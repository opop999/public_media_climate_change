---
title: "Word frequencies"
---
Load necessary packages
```{r include=FALSE}
# Package names
packages <-
  c(
    "dplyr",
    "stringr",
    "purrr",
    "tidyr",
    "tidytext",
    "quanteda",
    "jsonlite",
    "ggplot2",
    "data.table",
    "plotly",
    "forcats",
    "ggwordcloud",
    "ggpubr"
  )

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

```{r}
library(parallel)

list_of_processed_chunks <- list.files(path = "../2.data_transformations/data/udpipe_processed/", pattern = "*.rds", full.names = TRUE)

terms <- c("klima", "klimatický")

process_df <- function(chunk_path) {
  chunk_path %>% 
        readRDS() %>% 
        transmute(doc_id, lemma = tolower(lemma)) %>% 
        filter(lemma %in% terms)
}

climate_terms <- mclapply(list_of_processed_chunks, process_df, mc.cores = detectCores() - 1) %>% 
  bind_rows()

saveRDS(climate_terms, "data/climate_terms.rds")

```

```{r}
# Get info on doc_id and date of publication
list_of_full_chunks <- list.files(path = "../1.data_extraction/data/full_articles/",
                             pattern = "*.rds",
                             full.names = TRUE)

process_df_full <- function(chunk_path) {
  chunk_path %>% 
        readRDS() %>% 
        transmute(date = as.Date(PublishDate),
                  doc_id = Code)
}

doc_id_by_date <- mclapply(list_of_full_chunks, process_df_full, mc.cores = detectCores()) %>% 
  bind_rows() %>% 
  distinct()

saveRDS(doc_id_by_date, "data/doc_id_by_date.rds")

```

```{r}
doc_id_by_date <- readRDS("data/doc_id_by_date.rds")

climate_terms <- readRDS("data/climate_terms.rds") %>% 
  distinct() # To remove duplicate rows, i.e. to only get the number of documents where these terms appear


# Graph zoom date end & beginning
start_date <- as.Date("2012-01-01")
end_date <- as.Date("2022-04-30")

# Frequency of selected lemmata by month
frequency_summary_climate_terms <- climate_terms %>% 
  inner_join(doc_id_by_date, by = "doc_id") %>%
  mutate(month = as.Date(cut(date, "months"))) %>% 
  count(month, lemma, name = "monthly_count")

# Grouped counts of all selected lemma by month
climate_term_articles_per_month <- climate_terms %>% 
  inner_join(doc_id_by_date, by = "doc_id") %>%
  select(-lemma) %>% 
  distinct() %>% 
  mutate(month = as.Date(cut(date, "months"))) %>% 
  count(month, name = "monthly_count_climate")

# Summarize the overall content per media per month
all_articles_per_month <- doc_id_by_date %>%
  mutate(month = as.Date(cut(date, "months"))) %>% 
  count(month, name = "monthly_count_all") 

# Combine both datasets
combined_counts_per_month <- inner_join(all_articles_per_month, climate_term_articles_per_month, by = c("month" = "month")) %>%
      mutate(climate_prop = monthly_count_climate/monthly_count_all)

# Save data where each row is a month for each media
saveRDS(combined_counts_per_month, "data/combined_counts_per_month.rds")

```

```{r}
# Convenience function to give ceiling to decimals for graph size purposes
ceiling_dec <- function(x, level=1) round(x + 5*10^(-level-1), level)

```



```{r}
ggplot(frequency_summary_climate_terms, aes(x = month, y = monthly_count, color = lemma)) +
  geom_line() +
  geom_point(size = 0.3) +
  ylab("Document counts") +
  xlab(element_blank()) +
  labs(title = "Decade of Czech Television news",
       subtitle = "Total monthly counts of documents containing of at least one of the lemmatized terms, 2012-2022",
       caption = "Source: Newton Media Archive, January 2012-April 2022. N=531,592.") +
  theme_bw() +
  scale_color_manual(values = c("#0d7414", "#4974a5")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 7),
        plot.background = element_rect(fill = "grey90"),
        plot.title = element_text(face = "bold", size = 10),
        plot.subtitle = element_text(face = "italic", size = 8),
        plot.caption = element_text(size = 5),
        axis.title.y = element_text(size = 8),
        legend.title = element_blank(),
        legend.key = element_rect(fill = NA, size = 8),
        legend.text = element_text(size = 9),
        legend.background = element_rect(fill = NA),
        legend.position = c(0.85, 0.9),
        plot.margin = margin(7,30,3,5, "pt")) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%y") +
  coord_cartesian(xlim = c(start_date, end_date + 120), expand = FALSE) +
  scale_y_continuous(
  expand = c(0, 0),
  breaks = seq(0, 1000, 10),
  labels = seq(0, 1000, 10),
  limits = c(0, max(frequency_summary_climate_terms$monthly_count) + 20)
  )

ggsave("visuals/selected_lemma_frequencies.png", device = "png",
       width = 1920, height = 1080, units = "px")
```

```{r}
combined_counts_per_month %>% 
  ggplot(aes(x = month, y = climate_prop)) +
  geom_line(color = "#13a576") +
  geom_point(color = "#13a576", size = 0.3) +
  ylab(element_blank()) +
  xlab(element_blank()) +
  labs(title = "Decade of Czech Television news",
       subtitle = "Monthly proportion of overall documents containing at least one of the climate-terms",
       caption = "Source: Newton Media Archive, January 2012-April 2022. N=531,592. Selected lemmata: 'klima' & 'klimatický'") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 7),
        plot.background = element_rect(fill = "grey90"),
        plot.title = element_text(face = "bold", size = 10),
        plot.subtitle = element_text(face = "italic", size = 8),
        plot.caption = element_text(size = 4.5),
        axis.title.y = element_text(size = 8),
        legend.title = element_blank(),
        plot.margin = margin(7, 30, 3, 0, "pt")) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%y") +
  coord_cartesian(xlim = c(start_date, end_date + 120), expand = FALSE) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = seq(0, 1, 0.01),
    labels = paste0(seq(0, 100, 1), "%"),
    limits = c(0, ceiling_dec(max(combined_counts_per_month$climate_prop), 2))
  )

ggsave("visuals/climate_lemma_proportions.png", device = "png",
       width = 1920, height = 1080, units = "px")
```

```{r}
data.frame(variable = names(table(climate_terms$lemma)), count_1000 = round(as.numeric(table(climate_terms$lemma)/1000), 1)) %>%
  add_row(variable = "all_documents", count_1000 = round(nrow(doc_id_by_date)/1000, 1)) %>%
  mutate(variable = reorder(as.factor(variable), desc(count_1000))) %>%
  ggplot(aes(x = variable, y = count_1000, fill = variable)) +
  geom_col() +
  ylab(element_blank()) +
  xlab(element_blank()) +
  labs(title = "Decade of Czech Television news",
       subtitle = "Documents (in thousands) containing selected lemma compared to all documents, 2012-2022",
       caption = "Source: Newton Media Archive, January 2012-April 2022. N=531,592.") +
  theme_bw() +
  theme(
    axis.text = element_text(size = 7),
    plot.background = element_rect(fill = "grey90"),
    plot.title = element_text(face = "bold", size = 10),
    plot.subtitle = element_text(face = "italic", size = 8),
    plot.caption = element_text(size = 4.5),
    axis.title.y = element_text(size = 8),
    legend.position = "none",
    plot.margin = margin(7, 30, 3, 0, "pt")
  ) +
  scale_fill_manual(values = c("#0F428A", "#009087", "#D53F3A")) +
  scale_y_continuous(
  expand = c(0, 0),
  breaks = seq(0, 1000, 50),
  labels = seq(0, 1000, 50),
  limits = c(0, nrow(doc_id_by_date)/1000 + 50)
  )

ggsave("visuals/climate_lemma_counts.png", device = "png",
       width = 1920, height = 1080, units = "px")
  
```

