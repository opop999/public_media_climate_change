{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install pyreadr transformers torch datasets ","metadata":{"_cell_guid":"5e178ad6-72d1-45dd-a32b-5f73cb50d0ec","_uuid":"a3258195-88ae-44c0-88d7-52fc80b1f27f","collapsed":false,"execution":{"iopub.execute_input":"2022-06-27T15:42:46.451891Z","iopub.status.busy":"2022-06-27T15:42:46.450936Z","iopub.status.idle":"2022-06-27T15:44:11.091105Z","shell.execute_reply":"2022-06-27T15:44:11.090042Z"},"executionInfo":{"elapsed":5033,"status":"ok","timestamp":1656275533572,"user":{"displayName":"Ondřej Pekáček","userId":"15262765153837785656"},"user_tz":-120},"id":"Hc10Pywa0Huu","jupyter":{"outputs_hidden":false},"outputId":"ba25abea-096e-44d8-8e59-8cabb846df98","papermill":{"duration":84.654883,"end_time":"2022-06-27T15:44:11.093973","exception":false,"start_time":"2022-06-27T15:42:46.439090","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\nimport os\nfrom pathlib import Path\nfrom time import time\nfrom json import dump\n# import gdown\n# from kaggle_secrets import UserSecretsClient\nfrom torch import cuda\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nfrom pyreadr import read_r\nfrom datasets import Sequence, Features, Value, Dataset\nfrom tqdm.notebook import tqdm\n\n# Specify path to input and output files\nPATH_TO_INPUT = \"../input/climate-sent/\"\nPATH_TO_OUTPUT = \"output/\"\nMODEL_PATH = \"model/\"\nMODEL_URL = \"https://air.kiv.zcu.cz/public/CZERT-B_fb.zip\"\nYEAR_FILTER = [\"2022\"]\nDEVICE = 0 if cuda.is_available() else -1\nBATCH_BY = 4\n","metadata":{"_cell_guid":"c7c28722-9eb1-429b-a306-05573509b0a7","_uuid":"7272ac55-020b-4fa2-9c9e-2c64b018e021","collapsed":false,"execution":{"iopub.execute_input":"2022-06-27T15:44:11.164772Z","iopub.status.busy":"2022-06-27T15:44:11.163888Z","iopub.status.idle":"2022-06-27T15:44:26.132023Z","shell.execute_reply":"2022-06-27T15:44:26.130754Z"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1656278768380,"user":{"displayName":"Ondřej Pekáček","userId":"15262765153837785656"},"user_tz":-120},"id":"ukaLqkpj0xzD","jupyter":{"outputs_hidden":false},"papermill":{"duration":15.006167,"end_time":"2022-06-27T15:44:26.134591","exception":false,"start_time":"2022-06-27T15:44:11.128424","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate the sentiment analysis workflow into reusable functions.","metadata":{}},{"cell_type":"code","source":"def get_gpu_info():\n    \"\"\"Get the GPU information, if available.\"\"\"\n    if cuda.is_available():\n        print(\"GPU acceleration available\")\n        print(f\"\\nNr. of CUDA devices: {cuda.device_count()}\")\n        print(f\"\\nCurrent device nr.: {cuda.current_device()}\")\n        print(f\"\\nCurrent device name: {cuda.get_device_name(0)}\")\n        print(f\"\\nCurrent device properties: {cuda.get_device_properties(0)}\")\n        # Also, we could check for more details about the GPU:\n        print(os.system(\"nvidia-smi\"))\n    else:\n        print(\"GPU acceleration not available\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input_and_output(input_dir: str, output_dir: str, gdrive_url: str = None, overwrite_input: bool = False, overwrite_output: bool = False) -> None:\n    \n    if not os.path.exists(input_dir) or overwrite_input:\n        os.makedirs(input_dir)\n    if not os.path.exists(output_dir) or overwrite_output:\n        os.makedirs(output_dir)\n    print(\"Input and output directories should be ready.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_download(model_path: str, model_url: str, overwrite_existing: bool = False) -> None:\n    \"\"\"Downloads selected model from specified url if it does not exist already.\n\n    Args:\n        model_path (str): Path to the model.\n        model_url (str): Url to the model.\n        overwrite_existing (bool, optional): Defaults to False.\n    \"\"\"\n    if not os.path.exists(model_path) or overwrite_existing:\n        os.system(f\"\"\"\n        mkdir -p {model_path}\n        wget -nv {model_url} -O {model_path}/model.zip\n        unzip -j -d {model_path} {model_path}/model.zip\n        rm {model_path}/model.zip\n        \"\"\")\n        print(f\"Model downloaded to folder {model_path}\")\n    else:\n        print(\"Model already downloaded.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def configure_analytical_pipeline(model_path: str, processing_device: int):\n    \"\"\"Configures the pipeline for the sentiment analysis using the specified model.\n\n    Args:\n        model_path (str): Path to the model.\n        processing_device (int): Device to use for processing. 0 for GPU, -1 for CPU.\n\n    Returns:\n        _type_: Pipeline\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(\n        pretrained_model_name_or_path=model_path,\n        model_max_length=512)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_model_name_or_path=model_path)\n    return pipeline(\"sentiment-analysis\",\n                    model=model,\n                    tokenizer=tokenizer,\n                    device=processing_device,\n                    max_length=512,\n                    padding=\"longest\",\n                    truncation=True) # Current version of transformers at Kaggle does not support top_k=n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_only_new_files(path_to_input: str, path_to_output: str) -> list:\n    \"\"\"Get all files in the input directory that are not already in the output directory.\n\n    Args:\n        path_to_input (str): Path to the input directory.\n        path_to_output (str): Path to the output directory.\n\n    Returns:\n        list: List of files in the input directory that are not already in the output directory.\n    \"\"\"\n    if not os.path.exists(path_to_output):\n        # create directory if it doesn't exist\n        os.makedirs(path_to_output)\n    existing_processed_files = {Path(file.replace(\"sentiment_\", \"\")).stem for file in os.listdir(\n        path_to_output) if file.endswith((\".rds\", \".json\"))}\n    return sorted(({Path(file).stem for file in os.listdir(\n        path_to_input) if file.endswith(\".rds\")} - existing_processed_files))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_by_years(input_files: list, year_filter: list) -> tuple:\n    \"\"\"Filter the files by the specified years.\n\n    Args:\n        input_files (list): List of files in the input directory.\n        year_filter (list): List of years to filter by.\n\n    Returns:\n        tuple: Tuple of filtered files.\n    \"\"\"\n    return tuple(file for file in input_files if any(\n        year in file for year in year_filter))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outer loop over the files in the input directory.\ndef sentiment_analysis_workflow(path_to_input: str,\n                                path_to_output: str,\n                                model_pipeline,\n                                input_files_filtered: tuple,\n                                batch_by: int = 1) -> None:\n    \"\"\"Performs the sentiment analysis workflow on selected files from the input directory.\n\n    Args:\n        path_to_input (str): Path to the input directory.\n        path_to_output (str): Path to the output directory.\n        model_pipeline (_type_): Pipeline for the sentiment analysis.\n        input_files_filtered (tuple): Tuple of filtered files.\n        batch_by (int, optional): Size of the batch send to the model pipeline. Defaults to 1.\n    \"\"\"\n    print(\n        f\"Starting sentiment analysis workflow on {len(input_files_filtered)} files.\")\n    for count, file in enumerate(input_files_filtered, start=1):\n        # Reading one chunk from the input directory\n        udpipe_chunk = read_r(f\"{path_to_input + file}.rds\")[None][[\"doc_id\", \"sentence_id\", \"token\"]] \\\n            .groupby(['doc_id', \"sentence_id\"], sort=False, as_index=False) \\\n            .agg(\n                tokens=(\"token\", \" \".join)) \\\n            .groupby(['doc_id'], sort=False, as_index=False) \\\n            .agg(\n                text=(\"tokens\", list)\n        )\n            \n        udpipe_chunk = Dataset.from_pandas(udpipe_chunk, features=Features(\n            {\"doc_id\": Value(dtype=\"string\", id=None),\n             \"text\": Sequence(feature=Value(dtype='string', id=None),\n                              length=-1, id=None)}))\n\n        print(\n            f\"\"\"Starting sentinment analysis for chunk {file}.\n            File contains {udpipe_chunk.shape[0]} articles.\"\"\", flush=True)\n        # Inner loop over the individual texts in the dataframe. We can use batching,\n        # but for longer text sizes, it does not seem to make much (if any) difference.\n        sentiment_dict = {document[\"doc_id\"]: model_pipeline(\n            document[\"text\"], batch_size=batch_by) for document in tqdm(udpipe_chunk)}\n        \n        with open(file=f\"{path_to_output}sentiment_{file}.json\", mode=\"w\", encoding=\"utf8\") as out:\n            dump(sentiment_dict, out, sort_keys=False)\n        \n        print(\n            f\"Finished the sentiment analysis of the chunk {file}, nr. {count} out of {len(input_files_filtered)}.\",\n            flush=True)\n        print('All changes made in this Kaggle session should now be visible locally.', flush=True)\n# End of outer loop.\n    print(\"Finished the sentiment analysis of all chunks.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run specified functions with selected parameters.","metadata":{}},{"cell_type":"code","source":"# Detect if GPU availabe\nget_gpu_info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepare_input_and_output(input_dir=PATH_TO_INPUT, output_dir=PATH_TO_OUTPUT)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the model, run once\nmodel_download(\n    model_path=MODEL_PATH,\n    model_url=MODEL_URL,\n    overwrite_existing=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose pipeline based on whether GPU is available. 0 and higher are CUDA devices and -1 is CPU\nmodel_configured = configure_analytical_pipeline(\n    model_path=MODEL_PATH, processing_device=DEVICE)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"udpipe_files = get_only_new_files(\n    path_to_input=PATH_TO_INPUT, path_to_output=PATH_TO_OUTPUT)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"udpipe_files_filtered = filter_by_years(\n    input_files=udpipe_files, year_filter=YEAR_FILTER)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time()\nsentiment_analysis_workflow(\n    path_to_input=PATH_TO_INPUT,\n    path_to_output=PATH_TO_OUTPUT,\n    model_pipeline=model_configured,\n    input_files_filtered=udpipe_files_filtered,\n    batch_by=BATCH_BY)\nprint(f\"Finished sentiment analysis in {time() - start_time} seconds.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.system(f\"rm -rf {MODEL_PATH}\")","metadata":{},"execution_count":null,"outputs":[]}]}