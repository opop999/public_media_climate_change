---
title: "Topic modeling"
---

## Load necessary packages
```{r include=FALSE}
# Package names
packages <- c("dplyr", "parallel", "stringr", "text2map", "data.table", "dtplyr", "text2vec", "LDAvis", "jsonlite", "tidyr", "DT", "ggplot2", "gistr", "tidytext", "RcppML")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

```


```{r}
udpipe_chunks <- list.files(path = "../../2.data_transformations/data/udpipe_processed/", pattern = "*.rds", full.names = TRUE)

process_df <- function(i) {
  
  readRDS(i) %>% 
    lazy_dt() %>% 
    filter(upos %in% c("NOUN", "ADJ", "PROPN")) %>%
    filter(!grepl("NameType=Giv", feats, fixed = TRUE)) %>%
    transmute(doc_id, lemma = gsub("[^ěščřžýáíéóúůďťňa-z ]", " ", tolower(str_replace_na(lemma, replacement = "")))) %>% 
    filter(nchar(lemma) > 1) %>% # filter one letter words 
    group_by(doc_id) %>%
    summarize(text = str_squish(str_c(lemma, collapse = " "))) %>%
    ungroup() %>% 
    as_tibble()

}

combined_df <- mclapply(udpipe_chunks, process_df, mc.cores = detectCores() - 1) %>% bind_rows()

# combined_df <- combined_df %>% 
#   group_by(doc_id) %>%
#   summarize(text = str_squish(str_c(lemma, collapse = " "))) %>%
#   ungroup() %>% 
#   as_tibble()

```


```{r}
# To create a stop list of journalists of Czech television, we try to extract them from the full text.
full_chunks_paths <- list.files(path = "../../1.data_extraction/data/full_articles", pattern = "*.rds", full.names = TRUE)

get_tv_presenters <- function(full_chunk_path) {
  readRDS(full_chunk_path) %>% 
    transmute(presenter_name = tolower(word(gsub("<.*>", "", str_extract(Content, "^([^,]+)")), start = 2L, end = 2L))) %>% 
    filter(!is.na(presenter_name) & !grepl("-", presenter_name) & nchar(presenter_name) >= 3) %>% 
    pull(presenter_name)
}

czech_tv_presenters <- mclapply(full_chunks_paths, get_tv_presenters,  mc.cores = detectCores() - 1) %>% unlist() %>% table() %>% sort(decreasing = TRUE) %>% head(100) %>% names()

saveRDS(czech_tv_presenters, "data/czech_tv_presenters.rds")

```


Pruning of the vocabulary
```{r}
dtm_udpipe <- dtm_builder(data = combined_df, text = "text", doc_id = "doc_id")

saveRDS(dtm_udpipe, "data/dtm_udpipe_adj_propn.rds")


# Load stop word lists
stop_words_cs <- fromJSON("data/stopwords_cs.json")
stop_words_czech_television <- c(readRDS("data/czech_tv_presenters.rds"), "moderátorka", "moderátor", "redaktorka", "redaktor")


# Remove terms that are in  
# low_tf_idf <- dtm_udpipe %>%
#   dtm_melter %>%
#   bind_tf_idf(term, doc_id, freq) %>% 
#   group_by(term) %>%
#   summarize(tf_idf_mean = mean(tf_idf)) %>%
#   slice_min(tf_idf_mean, prop = 0.05) %>%
#   pull(term)


# dtm_stats(dtm_udpipe)


dtm_less_sparse <- dtm_stopper(dtm_udpipe,
                               stop_list = c(stop_words_cs, stop_words_czech_television),
                               stop_hapax = TRUE,
                               stop_null = TRUE,
                               stop_docprop = c(0.005, 0.99),
                               # stop_docfreq = c(floor(nrow(dtm_udpipe) * 0.01), ceiling(nrow(dtm_udpipe) * 0.5))
                               )

# Check for presence of climate related items
colnames(dtm_less_sparse)[grep(x = colnames(dtm_less_sparse), pattern = "klim.*")]


dtm_stats(dtm_less_sparse)

saveRDS(dtm_less_sparse, "data/dtm_less_sparse_adj_propn.rds")

```

```{r}
topics_n <- seq(10, 50, 5)

lda_topic_modeling <- function(i) {
  
    lda_model <- LDA$new(n_topics = i)

    set.seed(3859L)

    doc_topic_distribution <- lda_model$fit_transform(
    x = dtm_less_sparse,
    n_iter = 1000,
    convergence_tol = 0.001,
    n_check_convergence = 10,
    progressbar = FALSE
  )
    
    saveRDS(doc_topic_distribution, paste0("data/models/doc_topic_distribution_adj_propn", i, ".rds"))

    saveRDS(lda_model, paste0("data/models/lda_model_adj_propn", i, ".rds"))
    
    lda_model$plot(as.gist = TRUE, open.browser = FALSE, browse = FALSE, description = paste("Climate LDA topic model with", i, "topics (with adjectives)"))
    
    rm(doc_topic_distribution, lda_model)
   
    gc()
  
}

invisible(mclapply(topics_n, lda_topic_modeling, mc.cores = detectCores() - 1))

```


```{r}
# The lower perplexity the better. 
# 535 for 10 topic vs 394 for 30 topic vs 334 for 50 topic.
perplexity(dtm_less_sparse, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = doc_topic_distribution)

# Proportion of topics in the corpus vizualization

doc_topic_distribution %>%
      colMeans() %>% 
      setNames(paste0("topic_", 1:length(.))) %>% 
      as_tibble(rownames = "topic") %>%
      mutate(topic = reorder(as.factor(topic), desc(value))) %>% 
      ggplot(aes(x = topic, y = value)) + 
        geom_bar(stat = "identity")

lda_model$get_top_words(lambda = 1) %>% as.data.frame() %>% setNames(paste0("topic_", 1:ncol(.))) %>% datatable() 

lda_model$plot(out.dir = "plot", as.gist = TRUE)

                           
# serVis("plot/lda.json", as.gist = TRUE, filename = "topic_20")
# 
# gist_create(file.path(out.dir, list.files(out.dir))), filename = "topic_20"))


barplot(
  doc_topic_distribution[1,],
  xlab = "topic",
  ylab = "proportion",
  ylim = c(0, 1),
  names.arg = 1:ncol(doc_topic_distribution)
)


# Calculation of metrics for k topics
# library(ldatuning)
# 
# result <- FindTopicsNumber(dtm_less_sparse,
#                            mc.cores = parallel::detectCores() - 1)
# FindTopicsNumber_plot(result)

```

```{r}
# Optional: remove the lowest 5% tf-idf scores https://arxiv.org/pdf/1701.03227.pdf



# dtm_tfidf[,dtm_tfidf[,colMeans(dtm_tfidf, na.rm = TRUE) > quantile(colMeans(dtm_tfidf , na.rm = TRUE), probs = 0.05, names = FALSE)]]
    
```

# https://github.com/zdebruine/RcppML
```{r}
# DEVEL version devtools::install_github("zdebruine/RcppML")   
# 

# topic_word_distribution_nfm[1,] %>% sort(decreasing = TRUE) %>% head(20)

# Calculate mean squared error for an NMF model
# evaluate(model_nfm, dtm_less_sparse)

# Function to plot NMF Model
plot_nmf <-
  function(model, dtm, topic_word_distribution = NULL, ...) {
    if ("LDAvis" %in% rownames(installed.packages())) {
      if (is.null(topic_word_distribution) |
          !is.matrix(topic_word_distribution)) {
        topic_word_distribution <- predict(model, dtm)
      }
      
      # FUN to normalize matrix
      norm_matrix <- function(m, norm = c("l1", "l2", "none")) {
        stopifnot(is(m, "matrix") || is(m, "sparseMatrix"))
        norm = match.arg(norm)
        if (norm == "none")
          return(m)
        norm_vec = switch(norm,
                          l1 = 1 / rowSums(m),
                          l2 = 1 / sqrt(rowSums(m ^ 2)))
        norm_vec[is.infinite(norm_vec)] = 0
        if (is(m, "sparseMatrix"))
          Diagonal(x = norm_vec) %*% m
        else
          m * norm_vec
      }
      
      
      # Viz
      json <- createJSON(
        phi = norm_matrix(topic_word_distribution, "l1"),
        theta = norm_matrix(model@w, "l1"),
        doc.length	= rowSums(dtm),
        vocab = dimnames(model@h)[[2]],
        term.frequency = colSums(dtm),
        lambda.step = 0.1,
        reorder.topics = TRUE,
        mds.method = jsPCA_robust
      )
      
      LDAvis::serVis(json, ...)
      
    }
    
  }
```


```{r}
topics_n <- seq(10, 50, 5)

tf_idf_model <- TfIdf$new()

dtm_tfidf <- fit_transform(dtm_less_sparse, tf_idf_model)

nmf_topic_modeling <- function(i) {
  
    model_nmf <- nmf(dtm_tfidf, k = i, seed = 3859L, maxit = 1000, tol = 0.001)
    topic_word_distribution <- predict(model_nmf, dtm_tfidf)
    
    saveRDS(topic_word_distribution, paste0("data/models/topic_word_distribution_adj_propn_", i, ".rds"))
    saveRDS(model_nmf, paste0("data/models/nmf_model_adj_propn_", i, ".rds"))
    
    plot_nmf(
    model = model_nmf,
    dtm = dtm_tfidf,
    topic_word_distribution = topic_word_distribution,
    as.gist = TRUE,
    open.browser = FALSE,
    browse = FALSE,
    description = paste("Climate NMF topic model with", i, "topics (with adjectives), TF-IDF adjustment.")
  )
    
    rm(topic_word_distribution, model_nmf)
    gc()
  
}

invisible(lapply(topics_n, nmf_topic_modeling))

```

Optional: subset by climate sub-corpus
```{r}
climate_doc_id <- readRDS("../../2.data_transformations/data/climate_sub_corpus/climate_article_ids.rds")

climate_df <- combined_df[combined_df$doc_id %in% climate_doc_id,]

climate_dtm <-
  dtm_builder(data = climate_df,
              text = "text",
              doc_id = "doc_id") %>%
  dtm_stopper(
    stop_list = c(stop_words_cs, stop_words_czech_television),
    stop_hapax = TRUE,
    stop_null = TRUE,
    stop_docprop = c(0.005, 0.99)
  )

# LDA
    climate_lda_model <- LDA$new(n_topics = 20)

    set.seed(3859L)

    doc_topic_distribution <- climate_lda_model$fit_transform(
    x = climate_dtm,
    n_iter = 1000,
    convergence_tol = 0.001,
    n_check_convergence = 10,
    progressbar = FALSE
  )

   climate_lda_model$plot(reorder.topics = TRUE) 

colSums(climate_dtm)["stát"]   

# NFM

tf_idf_model <- TfIdf$new()
climate_dtm_tfidf <- fit_transform(dtm_less_sparse, tf_idf_model)

climate_model_nmf <- nmf(climate_dtm_tfidf, k = 20, seed = 3859L, maxit = 1000, tol = 0.001)
topic_word_distribution <- predict(climate_model_nmf, climate_dtm_tfidf)

    
json_nmf <- createJSON(
        phi = norm_matrix(topic_word_distribution, "l1"),
        theta = norm_matrix(climate_model_nmf@w, "l1"),
        doc.length	= rowSums(climate_dtm),
        vocab = dimnames(climate_model_nmf@h)[[2]],
        term.frequency = colSums(climate_dtm),
        lambda.step = 0.1,
        reorder.topics = TRUE,
        mds.method = jsPCA_robust
      ) 


LDAvis::serVis(json_nmf)


# Optionally use ggplot to plot the intertopic distance
json_obj <- jsonlite::fromJSON(json_nmf) 
mds_data <- as_tibble(json_obj$mdsDat) # can also use a data.frame
ggplot(mds_data, aes(x, y)) + geom_point()

```


