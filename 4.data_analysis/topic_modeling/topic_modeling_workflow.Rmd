---
title: "Topic modeling"
---

## Load necessary packages
```{r include=FALSE}
# Package names
packages <- c("dplyr", "parallel", "stringr", "text2map", "data.table", "dtplyr", "text2vec", "LDAvis", "jsonlite", "tidyr", "DT", "ggplot2", "gistr", "tidytext")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

stop_words_cs <- c(fromJSON("data/irene_stopwords.json"),
                   fromJSON("data/stopwords_cs.json")) %>%
                   unique()

```


```{r}
udpipe_chunks <- list.files(path = "../../2.data_transformations/data/udpipe_processed/", pattern = "*.rds", full.names = TRUE)

process_df <- function(i) {
  
  readRDS(i) %>% 
    lazy_dt() %>% 
    filter(upos %in% c("NOUN")) %>%    
    transmute(doc_id, lemma = gsub("[^ěščřžýáíéóúůďťňa-z ]", " ", tolower(str_replace_na(lemma, replacement = "")))) %>% 
    filter(nchar(lemma) > 1) %>% # filter one letter words 
    group_by(doc_id) %>%
    summarize(text = str_squish(str_c(lemma, collapse = " "))) %>%
    ungroup() %>% 
    as_tibble()

}

combined_df <- mclapply(udpipe_chunks, process_df, mc.cores = detectCores() - 1) %>% bind_rows()


# combined_df <- combined_df %>% 
#   group_by(doc_id) %>%
#   summarize(text = str_squish(str_c(lemma, collapse = " "))) %>%
#   ungroup() %>% 
#   as_tibble()

```

```{r}

dtm_udpipe <- dtm_builder(data = combined_df, text = "text", doc_id = "doc_id")

saveRDS(dtm_udpipe, "data/dtm_udpipe.rds")

# Remove terms that are in  
low_tf_idf <- dtm_udpipe %>%
  dtm_melter %>%
  bind_tf_idf(term, doc_id, freq) %>% 
  group_by(term) %>%
  summarize(tf_idf_mean = mean(tf_idf)) %>%
  slice_min(tf_idf_mean, prop = 0.05) %>%
  pull(term)


# dtm_stats(dtm_udpipe)

dtm_less_sparse <- dtm_stopper(dtm_udpipe,
                               stop_list = c(low_tf_idf, stop_words_cs, "moderátorka", "moderátor", "redaktorka", "redaktor"),
                               stop_hapax = TRUE,
                               stop_null = TRUE,
                               stop_docprop = c(0.005, 0.99),
                               # stop_docfreq = c(floor(nrow(dtm_udpipe) * 0.01), ceiling(nrow(dtm_udpipe) * 0.5))
                               )

# Check for presence of climate related items
colnames(dtm_less_sparse)[grep(x = colnames(dtm_less_sparse), pattern = "klim.*")]


dtm_stats(dtm_less_sparse)

saveRDS(dtm_less_sparse, "data/dtm_less_sparse.rds")

```

```{r}
topics_n <- seq(10, 50, 5)

lda_topic_modeling <- function(i) {
  
    lda_model <- LDA$new(n_topics = i)

    set.seed(3859L)

    doc_topic_distribution <- lda_model$fit_transform(
    x = dtm_less_sparse,
    n_iter = 1000,
    convergence_tol = 0.001,
    n_check_convergence = 10,
    progressbar = FALSE
  )
    
    saveRDS(doc_topic_distribution, paste0("data/models/doc_topic_distribution_", i, ".rds"))

    saveRDS(lda_model, paste0("data/models/lda_model_", i, ".rds"))
    
    lda_model$plot(as.gist = TRUE, open.browser = FALSE, description = paste("Climate LDA topic model with", i, "topics"))
    
    rm(doc_topic_distribution, lda_model)
   
    gc()
  
}

invisible(mclapply(topics_n, lda_topic_modeling, mc.cores = detectCores() - 1))

```


```{r}
# The lower perplexity the better. 
# 535 for 10 topic vs 394 for 30 topic vs 334 for 50 topic.
perplexity(dtm_less_sparse, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = doc_topic_distribution)

# Proportion of topics in the corpus vizualization

doc_topic_distribution %>%
      colMeans() %>% 
      setNames(paste0("topic_", 1:length(.))) %>% 
      as_tibble(rownames = "topic") %>%
      mutate(topic = reorder(as.factor(topic), desc(value))) %>% 
      ggplot(aes(x = topic, y = value)) + 
        geom_bar(stat = "identity")

lda_model$get_top_words(lambda = 1) %>% as.data.frame() %>% setNames(paste0("topic_", 1:ncol(.))) %>% datatable() 

lda_model$plot(out.dir = "plot", as.gist = TRUE)

                           
# serVis("plot/lda.json", as.gist = TRUE, filename = "topic_20")
# 
# gist_create(file.path(out.dir, list.files(out.dir))), filename = "topic_20"))


barplot(
  doc_topic_distribution[1,],
  xlab = "topic",
  ylab = "proportion",
  ylim = c(0, 1),
  names.arg = 1:ncol(doc_topic_distribution)
)


# Calculation of metrics for k topics
# library(ldatuning)
# 
# result <- FindTopicsNumber(dtm_less_sparse,
#                            mc.cores = parallel::detectCores() - 1)
# FindTopicsNumber_plot(result)

```

```{r}
# Optional: remove the lowest 5% tf-idf scores https://arxiv.org/pdf/1701.03227.pdf
dtm_tfidf[,dtm_tfidf[,colMeans(dtm_tfidf, na.rm = TRUE) > quantile(colMeans(dtm_tfidf, na.rm = TRUE), probs = 0.05, names = FALSE)]]
    
```

