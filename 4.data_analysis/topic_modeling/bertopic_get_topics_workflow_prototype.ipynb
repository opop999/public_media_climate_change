{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv(\"../../sentences_with_dates.csv\")[\"sentence\"].tolist()\n",
    "# data_full = pd.read_csv(\"test_docs_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \"text\" column of the \"data\" dataframe to list\n",
    "# docs_climate = data[\"sentence\"].tolist()\n",
    "# docs_climate = data_full[\"text\"].tolist()\n",
    "\n",
    "# Summary statistics on the docs list\n",
    "print(f\"Number of documents: {len(docs_climate)}\")\n",
    "print(f\"Number of unique documents: {len(set(docs_climate))}\")\n",
    "print(\n",
    "    f\"Number of documents with length 0: {len([docs_climate for docs_climate in docs_climate if len(docs_climate) == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if all elements of docs_climate list are strings. If not, convert them to strings.\n",
    "if not all(isinstance(sentence, str) for sentence in sentences):\n",
    "    sentences = [str(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# Load json file with stopwords as a list\n",
    "with open(\"data/stopwords_cs.json\", \"r\") as f:\n",
    "    stopwords_cs = json.load(f)\n",
    "    \n",
    "stopwords_cs.extend([\"moderátorka\", \"moderátor\", \"redaktorka\", \"redaktor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_umap_model = UMAP(random_state=3859)\n",
    "vectorizer_model = CountVectorizer(stop_words=stopwords_cs)\n",
    "topic_model = BERTopic(embedding_model=sentence_model, umap_model=custom_umap_model, min_topic_size = 1000, calculate_probabilities=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1 = sentence_model.encode([\"Ahoj.\",  \"Co je.\", \"Jak se máte.\",  \"čau lidi.\"])\n",
    "array_2 = np.concatenate((sentence_model.encode([\"Ahoj.\",  \"Co je.\"]), sentence_model.encode([\"Jak se máte.\",  \"čau lidi.\"])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m climate_embeddings\u001b[39m.\u001b[39;49msave(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/climate_embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "climate_embeddings.save(filename=\"data/climate_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs_climate, data[\"date\"].tolist(), nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()\n",
    "# topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(n_words=30, width=400, height=500, top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If probabilities were calculated, then the topics can be visualized with the following code:\n",
    "# topic_model.visualize_distribution(probs[0])\n",
    "topic_model.get_representative_docs(2)\n",
    "topic_model.visualize_heatmap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fcc423611767790f2393ecee3bb6574101f9de8433eefd3b59182ef24a3412e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
