# This R Markdown file contains code for performing Key Word in Context (KWIC)/concordances analysis. 
# It loads necessary libraries, sets parameters to adjust, specifies regex pattern to match the key words of interest, 
# processes chunks individually with get_kwics function to get combined dataset, and creates a wordcloud visualization. 
# The code is divided into several code chunks, each with its own purpose. 
---
title: "Key Word in Context (KWIC)/concordances analysis"
---

# Load necessary libraries
```{r}
packages <-
  c(
    "stringr",
    "purrr",
    "tidyr",
    "quanteda",
    "wordcloud",
    "parallel",
    "data.table",
    "dtplyr",
    "dplyr"
  )

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

# Load main function
source("get_kwics.R")

```

# Parameters to adjust
```{r}
path_to_udpipe_chunks <- "../../2.data_transformations/data/udpipe_processed"

list_of_udpipe_chunks <- list.files(path = path_to_udpipe_chunks,
                                    pattern = "*.rds",
                                    full.names = TRUE)
# Do you only want certain time periods to be included? Uncomment and pipe the line below.
                                  # .[grepl("2015-(10|11|12)", .)]

# Should tokens or lemma be analyzed? Choosing lemma improves performance at the cost of readability.
lemma_or_token <- "token" # lemma / token
# Should the results be lowercased? Might improve performance at the cost of readability.
lowercase <- FALSE
# Size of the context window in which concordances will be found.
max_words_context <- 5 
# Which UPOS do we include? The fewer the faster the script.
# By default, we exclude symbols, prepositions, connections, particles, interjections.
upos_filter <-
  c(
    "VERB",
    "NOUN",
    "ADJ",
    "PROPN",
    "PUNCT",
    "ADV",
    "NUM",
    "AUX",
    "PRON",
    # "ADP",
    # "CCONJ",
    # "SCONJ",
    # "INTJ",
    # "PART",
    # SYM,
    "DET"
  )
# Choose which columns will the wordloud visualize. By default, we include both. 
context_direction <- c("pre", "post") 
```

# Specify regex pattern to match the key words of interest
```{r}
# Notes on the variables: 
# filter_main removes from the keyword column. For instance, we have matched
# "klima" for the KWIC analysis, but "klimatizace" got matched as well. This will 
# remove it.
# filter_pre and filter_post remove from the pre and post columns.
# For instance, we want to delete all rows, where "societal" OR "political" 
# preceeds the word "climate", as these phrases have nothing to do with climate change. 
kwic_pattern_regex <- list(climate = c(key_word = phrase(c("\\bklima\\S*")),
                                       # We filter "klimatiz*" OR "klimatolo*" from main keywords
                                       filter_main = "klimatiz\\S+|klimatolo\\S+",
                                       # To match immediately preceeding word to the keyword in pre column, "$" must be used
                                       filter_pre = "(společensk\\S+|politick\\S+|podnikatelsk\\S+)$",
                                       # To match immediately following word in post column, "^" must be used
                                       filter_post = "^ve společnost\\S+"), 
                                       # We use phrase() function with ordinary space between both terms
                        global_warming = c(key_word = phrase(c("\\bglobáln\\S+ oteplován\\S+")),
                                           filter_main = NA,
                                           filter_pre = NA,
                                           filter_post = NA),
                        greenhouse_effect = c(key_word = phrase(c("\\bskleníkov\\S+ efekt\\S*")),
                                              filter_main = NA,
                                              filter_pre = NA,
                                              filter_post = NA),
                        carbon_footprint = c(key_word = phrase(c("\\buhlíkov\\S+ stop\\S+")),
                                             filter_main = NA,
                                             filter_pre = NA,
                                             filter_post = NA))
```

# Process chunks individually with get_kwics function to get combined dataset
```{r}
# If we are using Linux or MacOS, we can use multiple CPUs to make the whole
# process much faster. Each core handles different chunk at the same time.

# Check if list_of_udpipe_chunks is not empty and files exist
if (length(list_of_udpipe_chunks) == 0) {
  stop("No udpipe processed files found in the specified directory.")
} else if (!all(file.exists(list_of_udpipe_chunks))) {
  stop("Some files from the specified list do not exist.")
}

if (Sys.info()[['sysname']] == "Windows") {
  # Use normal apply with windows
  kwic_df_full <-
    lapply(list_of_udpipe_chunks, get_kwics) %>% bind_rows()

} else {
  kwic_df_full <-
    mclapply(list_of_udpipe_chunks, get_kwics, mc.cores = detectCores() - 2) %>% bind_rows()
}

saveRDS(kwic_df_full, "data/kwic_df_full.rds")
```

# Create wordcloud visualization
```{r}
# Wordcloud of all of the pre & post words: Automatically splits multi-word token elements
kwic_df_full %>%
  unite(col = "all_context", all_of(context_direction), sep = " ", na.rm = TRUE) %>% # Combine "pre" and "post" columns into 'all_context' column
  pull(all_context) %>% # Keep only the 'all_context' column
  strsplit(split = " ") %>% # Split the 'all_context' column by space, creating a list of vectors
  unlist() %>% # Unlist the list of vectors into a single vector
  tibble(word = .) %>% # Create a tibble with a single column 'word'
  count(word, sort = TRUE) %>% # Count the number of occurrences of each word
  filter(!is.na(word)) %>% # Remove NA values
  with(wordcloud(word, n, min.freq = 500, random.order = FALSE, max.words = 100, colors = brewer.pal(8, "Dark2")))
```

