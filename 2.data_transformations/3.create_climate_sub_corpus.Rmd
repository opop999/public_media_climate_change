---
title: "Sub-corpus creation"
---
Load necessary packages
```{r include=FALSE}
# Package names
packages <- c("dplyr", "stringr", "readr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))

# Regex search string based on suggestions from area experts
climate_regex_string <- "klima\\S*|antropocén|permafrost|nízkoemisn\\S+|uhlíkov\\S*|větrn\\S+|IPCC|UNFCCC|fosiln\\S+"

# Optionally specify negative regex search string to improve performance
negative_climate_regex_string <- "klimatiza\\S*|povětrn\\S*"

path_to_input <- "data/udpipe_processed"

```

```{r}
# Identify UPIPE-processed chunks of interest
udpipe_chunks_path <- list.files(path = file.path(path_to_input),
                              pattern = "*.rds",
                              full.names = TRUE)

# Define function, which loads every chunk and extracts all of the documents that 
# match the regex search string
get_climate_articles <- function(udpipe_chunk_path) {
  
  readRDS(udpipe_chunk_path) %>%
    mutate(lemma = str_to_lower(lemma, locale = "cs")) %>%
    filter(
      str_detect(lemma, climate_regex_string) &
        str_detect(lemma, negative_climate_regex_string, negate = TRUE)
    ) %>%
    pull(doc_id) %>%
    unique()
  
}
```

```{r}
# Iterate over the list of UDPIPE chunks
# If on Linux, we use the parallelized version of lapply
if (Sys.info()[['sysname']] == "Linux") {
  
library(parallel)

climate_article_ids <- mclapply(udpipe_chunks_path, get_climate_articles,  mc.cores = detectCores() - 2) %>% unlist()

} else {
# If not Linux - non-parallelized lapply

climate_article_ids <- lapply(udpipe_chunks_path, get_climate_articles) %>% unlist()

}

saveRDS(climate_article_ids, "data/climate_sub_corpus/climate_article_ids.rds")

```

Optional: Export sub-corpus with texts
```{r}

climate_article_ids <- readRDS("data/climate_sub_corpus/climate_article_ids.rds")

path_to_input_regex <- "data/regex_processed"

# Identify Regex-processed chunks of interest
regex_chunks_path <- list.files(path = file.path(path_to_input_regex),
                              pattern = "*.rds",
                              full.names = TRUE)

get_climate_texts <- function(regex_chunks_path) {
  readRDS(regex_chunks_path) %>%
    filter(article_id %in% climate_article_ids)
}


if (Sys.info()[['sysname']] == "Linux") {
  
library(parallel)

climate_articles <- mclapply(regex_chunks_path, get_climate_texts,  mc.cores = detectCores() - 2) %>% bind_rows()

} else {
# If not Linux - non-parallelized lapply
climate_articles  <- lapply(regex_chunks_path, get_climate_texts) %>% bind_rows()
}

# We probably need to observe the Google Sheets 50k character limit, otherwise the text of the article will not appear.
climate_articles$text <- str_trunc(str_squish(climate_articles$text), width = 49997, side = "right", ellipsis = "...")

# Excel formula to replicate
# =trim(regexreplace(regexreplace(lower(B2),".?(klima\S*|antropocén|permafrost|nízkoemisn\S+|uhlíkov\S*|větrn\S+|IPCC|UNFCCC|fosiln\S+)|.", "$1 "), "klimatiza\S*|povětrn\S*|\.", ""))

write_csv(climate_articles, "data/climate_sub_corpus/climate_articles.csv")

```


